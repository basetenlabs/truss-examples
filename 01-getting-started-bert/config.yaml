# # Step 2: Writing the config.yaml
#
# Each Truss has a config.yaml file where we can configure
# options related to the deployment. It's in this file where
# we can define requirements, resources, and runtime options like
# secrets and environment variables
#
# ### Basic Options
#
# In this section, we can define basic metadata about the model,
# such as the name, and the Python version to build with.
model_name: bert
python_version: py310
model_metadata:
  example_model_input: { "text": "Hello my name is {MASK}" }


# ### Set up python requirements
#
# In this section, we define any pip requirements that
# we need to run the model. To run this, we need PyTorch
# and Tranformers.
requirements:
  - torch==2.0.1
  - transformers==4.33.2

# ### Configure the resources needed
#
# In this section, we can configure resources
# needed to deploy this model. Here, we have no need for a GPU
# so we leave the accelerator section blank.
resources:
  accelerator: null
  cpu: '1'
  memory: 2Gi
  use_gpu: false

# ### Other config options
#
# Truss also has provisions for adding other runtime options
# packages. In this example, we don't need these, so we leave
# this empty for now.
secrets: {}
system_packages: []
environment_variables: {}
external_package_dirs: []

# # Step 3: Deploying & running inference
#
# Deploy the model with the following command:
#
# ```bash
# $ truss push
# ```
#
# And then you can performance inference with:
# ```
# $ truss predict -d '"Truss is awesome!"'
# ```
