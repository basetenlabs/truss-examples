model_metadata:
  example_model_input:
    messages:
      - role: system
        content: "You are a helpful assistant."
      - role: user
        content: "What is the meaning of life?"
    stream: true
    model: zai-org/GLM-5
    max_tokens: 32768
    temperature: 0.7
  tags:
    - openai-compatible

model_name: zai-org/GLM-5

base_image:
  image: vllm/vllm-openai:glm5

docker_server:
  start_command: >
    sh -c "VLLM_DEEP_GEMM_WARMUP=relax python3 -m vllm.entrypoints.openai.api_server
    --model /models/GLM-5-FP8
    --chat-template /models/GLM-5-FP8/chat_template.jinja
    --host 0.0.0.0 --port 8000
    --served-model-name zai-org/GLM-5
    --tensor-parallel-size 8
    --trust-remote-code
    --load-format runai_streamer
    --disable-log-stats
    --max-num-seqs 64
    --max-num-batched-tokens 8192
    --tool-call-parser glm47
    --reasoning-parser glm45
    --enable-auto-tool-choice
    --speculative-config.method mtp
    --speculative-config.num_speculative_tokens 1"
  readiness_endpoint: /health
  liveness_endpoint: /health
  predict_endpoint: /v1/chat/completions
  server_port: 8000

weights:
  - source: "hf://zai-org/GLM-5-FP8@main"
    mount_location: "/models/GLM-5-FP8"
    ignore_patterns:
      - "*.md"
      - "*.txt"

resources:
  accelerator: B200:8
  use_gpu: true

runtime:
  predict_concurrency: 64
  health_checks:
    restart_check_delay_seconds: 1800
    restart_threshold_seconds: 1200
    stop_traffic_threshold_seconds: 120
