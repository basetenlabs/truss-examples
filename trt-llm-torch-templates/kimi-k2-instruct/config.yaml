model_name: Kimi K2 Instruct Internal Cache Final
python_version: py39
resources:
  accelerator: B200:4
  cpu: "1"
  memory: 10Gi
  use_gpu: true
model_metadata:
  repo_id: baseten/Kimi-K2-Instruct-FP4
  example_model_input:
    model: baseten/Kimi-K2-Instruct-FP4
    messages:
      - role: user
        content: "Provide the FactSet details for Baseten.co latest earnings call."
trt_llm:
  build:
    checkpoint_repository:
      repo: baseten/Kimi-K2-Instruct-FP4
      revision: 70b0fd863895ef1bae0706471685292d6f63261c
      source: HF
  inference_stack: v2
  runtime:
    enable_chunked_prefill: false
    max_batch_size: 16
    max_num_tokens: 8192
    max_seq_len: 8192
    tensor_parallel_size: 4
    served_model_name: baseten/Kimi-K2-Instruct-FP4
    patch_kwargs:
      disable_overlap_scheduler: True
      guided_decoding_backend: null

      kv_cache_config:
        free_gpu_memory_fraction: 0.8
        enable_block_reuse: false

      autotuner_enabled: false
