model_name: LLama-4-maverick-h100 BISLLM
python_version: py39
resources:
  accelerator: H100:8
  cpu: "1"
  memory: 10Gi
  use_gpu: true
model_metadata:
  example_model_input:
    {
      "model": "meta-llama/Llama-4-Maverick-17B-128E-Instruct",
      "messages":
        [
          {
            "role": "user",
            "content": "Given an array of integers nums and an integer target, return indices of the two numbers such that they add up to target. You may assume that each input would have exactly one solution, and you may not use the same element twice. You can return the answer in any order. class Solution: def twoSum(self, nums: List[int], target: int) -> List[int]:",
          },
        ],
      "stream": true,
      "max_tokens": 2048,
      "temperature": 0.5,
    }
model_cache:
  - repo_id: nvidia/Llama-4-Maverick-17B-128E-Instruct-FP8
    revision: main
    use_volume: true
    volume_folder: llama
    runtime_secret_name: "hf_access_token"
    kind: "hf"
trt_llm:
  build:
    checkpoint_repository:
      repo: michaelfeil/empty-model
      revision: main
      source: HF
  inference_stack: v2
  runtime:
    enable_chunked_prefill: true
    max_batch_size: 16
    max_num_tokens: 8192
    max_seq_len: 340000
    tensor_parallel_size: 8
    served_model_name: meta-llama/Llama-4-Maverick-17B-128E-Instruct
    patch_kwargs:
      enable_trtllm_sampler: True
      guided_decoding_backend: "xgrammar"
      moe_expert_parallel_size: 4

      kv_cache_config:
        free_gpu_memory_fraction: 0.8
        enable_block_reuse: true

      cuda_graph_config:
        padding_enabled: true
      enable_iter_perf_stats: false
      autotuner_enabled: false
      tool_call_parser: pythonic

      # model cache
      model_path: /app/model_cache/llama
