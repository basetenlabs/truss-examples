base_image:
  image: vllm/vllm-openai:v0.12.0
model_metadata:
  example_model_input: {
    messages: [
      {
        role: "user",
        content: "Write me a short story about a cat."
      }
    ],
    stream: true,
    max_tokens: 512,
    temperature: 0.6
  }
  repo_id: nvidia/NVIDIA-Nemotron-3-Nano-30B-A3B-BF16
  tags:
    - openai-compatible
docker_server:
  start_command: sh -c "wget https://huggingface.co/nvidia/NVIDIA-Nemotron-3-Nano-30B-A3B-BF16/resolve/main/nano_v3_reasoning_parser.py -O /app/nano_v3_reasoning_parser.py && HF_TOKEN=$(cat /secrets/hf_access_token) vllm serve nvidia/NVIDIA-Nemotron-3-Nano-30B-A3B-BF16 --max-num-seqs 8 --tensor-parallel-size 1 --max-model-len 262144 --port 8000 --trust-remote-code --tool-call-parser qwen3_coder --reasoning-parser-plugin /app/nano_v3_reasoning_parser.py --reasoning-parser nano_v3"
  readiness_endpoint: /health
  liveness_endpoint: /health
  predict_endpoint: /v1/chat/completions
  server_port: 8000
resources:
  accelerator: H100
  use_gpu: true
runtime:
  predict_concurrency : 8
model_name: Nemotron 3 Nano
environment_variables:
  hf_access_token: null
system_packages:
  - wget
