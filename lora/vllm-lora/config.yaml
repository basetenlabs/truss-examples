base_image:
  image: vllm/vllm-openai:v0.9.2
model_metadata:
  example_model_input: {
    model: "finance",
    messages: [
      {
        role: "user",
        content: "How would you choose back in 2008?"
      }
    ],
    stream: true,
    max_tokens: 512,
    temperature: 0.9
  }
  repo_id: mistralai/Mistral-7B-Instruct-v0.3
docker_server:
  start_command: sh -c "HF_TOKEN=$(cat /secrets/hf_access_token) vllm serve mistralai/Mistral-7B-Instruct-v0.3 --tokenizer_mode mistral --config_format mistral --load_format mistral --served-model-name mistral --max-model-len 16384 --port 8000 --gpu-memory-utilization 0.90 --disable-custom-all-reduce --trust-remote-code --enable-lora --lora-modules finance=vaibhav1/lora-mistral-finance legal=Aretoss/Lexgen"
  readiness_endpoint: /health
  liveness_endpoint: /health
  predict_endpoint: /v1/chat/completions
  server_port: 8000
resources:
  accelerator: H100_40GB
  cpu: '1'
  memory: 24Gi
  use_gpu: true
runtime:
  predict_concurrency : 32
model_name: Mistral-7B-Instruct VLLM Lora
environment_variables:
  hf_access_token: null
