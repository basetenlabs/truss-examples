base_image:
  image: lmsysorg/sglang:v0.4.9.post6-cu126
model_metadata:
  example_model_input: {
    "text": [
        "What would you choose in 2008?",
        "What would you choose in 2008?",
    ],
    "sampling_params": {"max_new_tokens": 1000, "temperature": 1.0},
    "lora_path": ["legal", "finance"],
  }
  repo_id: mistralai/Mistral-7B-Instruct-v0.3
docker_server:
  start_command: sh -c "HF_TOKEN=$(cat /secrets/hf_access_token) python3 -m sglang.launch_server --model-path mistralai/Mistral-7B-Instruct-v0.3 --port 8000 --trust-remote-code --enable-lora --lora-paths legal=Aretoss/Lexgen finance=vaibhav1/lora-mistral-finance medical=Imsachinsingh00/Fine_tuned_LoRA_Mistral_MTSDialog_Summarization --disable-radix-cache"
  readiness_endpoint: /health
  liveness_endpoint: /health
  predict_endpoint: /generate
  server_port: 8000
resources:
  accelerator: H100_40GB
  cpu: '1'
  memory: 24Gi
  use_gpu: true
runtime:
  predict_concurrency : 32
model_name: Mistral-7B-Instruct SGLang Lora
environment_variables:
  hf_access_token: null
requirements:
  - protobuf
