model_metadata:
  example_model_input:
    model: "deepseek-ai/deepseek-vl2"
    messages:
      - role: user
        content:
          - type: image_url
            image_url:
              url: "data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mNk+M9QDwADhgGAWjR9awAAAABJRU5ErkJggg=="
          - type: text
            text: "<|grounding|>Convert the document to markdown."
    max_tokens: 4096
    temperature: 0.6
  tags:
    - openai-compatible
model_name: deepseek vl2
base_image:
  image: lmsysorg/sglang:v0.5.5
docker_server:
  start_command: sh -c "python3 -m sglang.launch_server --model deepseek-ai/deepseek-vl2 --served-model-name deepseek-ai/deepseek-vl2 --host 0.0.0.0 --port 8000"
  readiness_endpoint: /health_generate
  liveness_endpoint: /health_generate
  predict_endpoint: /v1/chat/completions
  server_port: 8000
resources:
  accelerator: H100
  use_gpu: true
runtime:
  predict_concurrency: 256
