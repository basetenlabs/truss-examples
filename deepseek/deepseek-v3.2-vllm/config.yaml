model_name: DeepSeek V3.2 vLLM
model_metadata:
  example_model_input:
    messages:
      - role: system
        content: "You are a helpful assistant."
      - role: user
        content: "Explain the concept of mixture of experts in neural networks."
    model: deepseek-ai/DeepSeek-V3.2
    stream: true
    max_tokens: 4096
    temperature: 0.7
  tags:
    - openai-compatible
base_image:
  image: vllm/vllm-openai:latest
docker_server:
  start_command: >
    sh -c "python3 -m vllm.entrypoints.openai.api_server
    --model /models/DeepSeek-V3.2-NVFP4
    --host 0.0.0.0 --port 8000
    --served-model-name deepseek-ai/DeepSeek-V3.2
    --tensor-parallel-size 8
    --trust-remote-code
    --tokenizer-mode deepseek_v32
    --max-num-seqs 64
    --max-num-batched-tokens 131072
    --max-model-len 131072
    --kv-cache-dtype fp8
    --gpu-memory-utilization 0.92
    --enable-chunked-prefill
    --enable-prefix-caching
    --disable-log-stats"
  readiness_endpoint: /health
  liveness_endpoint: /health
  predict_endpoint: /v1/chat/completions
  server_port: 8000
weights:
  - source: "hf://nvidia/DeepSeek-V3.2-NVFP4@main"
    mount_location: "/models/DeepSeek-V3.2-NVFP4"
    auth_secret_name: "hf_access_token"
resources:
  accelerator: B200:8
  cpu: "1"
  memory: 10Gi
  use_gpu: true
runtime:
  predict_concurrency: 64
  health_checks:
    restart_check_delay_seconds: 1800
    restart_threshold_seconds: 30
    stop_traffic_threshold_seconds: 120
