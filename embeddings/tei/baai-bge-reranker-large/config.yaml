# this file was autogenerated by `generate_templates.py` - please do change via template only
description: "BAAI/bge-reranker-large reranker model"
base_image:
  image: baseten/text-embeddings-inference-mirror:hopper-1.8.3
docker_server:
  liveness_endpoint: /health
  predict_endpoint: /rerank
  readiness_endpoint: /health
  server_port: 7997
  start_command: bash -c "truss-transfer-cli && text-embeddings-router --port 7997
    --model-id /app/model_cache/cached_model --max-client-batch-size 128 --max-concurrent-requests
    1024 --max-batch-tokens 16384 --auto-truncate"
model_cache:
- ignore_patterns:
  - '*.pt'
  - '*.ckpt'
  - '*.onnx'
  repo_id: BAAI/bge-reranker-large
  revision: main
  use_volume: true
  volume_folder: cached_model
model_metadata:
  example_model_input:
    query: What is Baseten?
    raw_scores: true
    return_text: true
    texts:
    - Deep Learning is ...
    - Baseten is a fast inference provider
    truncate: true
    truncation_direction: Right
model_name: TEI-baai-bge-reranker-large-truss-example
python_version: py39
resources:
  accelerator: H100
  cpu: '1'
  memory: 2Gi
  use_gpu: true
runtime:
  is_websocket_endpoint: false
  predict_concurrency: 32
  transport:
    kind: http
