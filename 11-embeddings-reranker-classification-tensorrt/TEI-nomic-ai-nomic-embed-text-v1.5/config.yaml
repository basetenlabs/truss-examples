# this file was autogenerated by `generate_templates.py` - please do change via template only
base_image:
  image: baseten/text-embeddings-inference-mirror:86-1.7.2
docker_server:
  liveness_endpoint: /health
  predict_endpoint: /v1/embeddings
  readiness_endpoint: /health
  server_port: 7997
  start_command: bash -c "truss-transfer-cli && text-embeddings-router --port 7997
    --model-id /app/model_cache/cached_model --max-client-batch-size 128 --max-concurrent-requests
    1024 --max-batch-tokens 16384 --auto-truncate --tokenization-workers 3"
model_cache:
- ignore_patterns:
  - '*.pt'
  - '*.ckpt'
  - '*.onnx'
  repo_id: nomic-ai/nomic-embed-text-v1.5
  revision: main
  use_volume: true
  volume_folder: cached_model
model_metadata:
  example_model_input:
    encoding_format: float
    input: text string
    model: model
model_name: TEI-nomic-ai-nomic-embed-text-v1.5-truss-example
python_version: py39
resources:
  accelerator: A10G
  cpu: '1'
  memory: 2Gi
  use_gpu: true
runtime:
  is_websocket_endpoint: false
  predict_concurrency: 32
  transport:
    kind: http
