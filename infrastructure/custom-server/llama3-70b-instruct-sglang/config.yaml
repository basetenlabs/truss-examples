description: "Llama 3.1 70B Instruct via custom SGLang server"
base_image:
  image: lmsysorg/sglang:v0.4.0.post1-cu124
model_metadata:
  example_model_input: {"model": "meta-llama/Llama-3.1-70B-Instruct", "prompt": "What is machine learning?", "max_tokens": 512}
  repo_id: meta-llama/Llama-3.1-70B-Instruct
docker_server:
  start_command: sh -c "HF_TOKEN=$(cat /secrets/hf_access_token) python3 -m sglang.launch_server --model-path meta-llama/Llama-3.1-70B-Instruct --port 8000 --tp 4"
  readiness_endpoint: /health
  liveness_endpoint: /health
  predict_endpoint: /v1/completions
  server_port: 8000
resources:
  accelerator: H100:4
  use_gpu: true
runtime:
  predict_concurrency : 32
model_name: Llama 3.1 70B Instruct SGLang
environment_variables:
  hf_access_token: null
