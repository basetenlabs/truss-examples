description: "N-gram speculative decoding example via custom server"
base_image:
  image: vllm/vllm-openai:nightly
model_metadata:
  repo_id: Qwen/Qwen3-0.6B
  example_model_input: {
    "model": "llama",
    "messages": [
      {
      "role": "user",
      "content": [
        {
        "type": "text",
        "text": "What do llamas dream of?"
        }
      ]
      }
    ],
    "stream": false,
    "max_tokens": 512,
  }
docker_server:
  start_command: sh -c "vllm serve distilbert/distilgpt2 --served-model-name llama --tensor-parallel-size 1 --max-model-len 64 --max-num-seqs 1 --max-num-batched-tokens 64 --gpu-memory-utilization 0.7"
  readiness_endpoint: /health
  liveness_endpoint: /health
  predict_endpoint: /v1/chat/completions
  server_port: 8000
runtime:
  predict_concurrency : 16
resources:
  accelerator: H100
  use_gpu: true
model_name: ngram-speculator
environment_variables:
  hf_access_token: null
