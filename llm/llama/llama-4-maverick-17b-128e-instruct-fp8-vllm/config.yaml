description: "meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8 for text generation"
base_image:
  image: vllm/vllm-openai:v0.8.4
build_commands:
  - pip install git+https://github.com/huggingface/transformers.git hf-xet
model_metadata:
  repo_id: meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8
  example_model_input: {
    "model": "meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8",
    "messages": [
      {
      "role": "user",
      "content": "Given an array of integers nums and an integer target, return indices of the two numbers such that they add up to target. You may assume that each input would have exactly one solution, and you may not use the same element twice. You can return the answer in any order. class Solution: def twoSum(self, nums: List[int], target: int) -> List[int]:"
      }
    ],
    "stream": true,
    "max_tokens": 512,
    "temperature": 0.5
  }
  tags:
  - openai-compatible
docker_server:
  start_command: sh -c /app/data/do.sh
  readiness_endpoint: /health
  liveness_endpoint: /health
  predict_endpoint: /v1/chat/completions
  server_port: 8000
environment_variables:
  VLLM_LOGGING_LEVEL: INFO
  hf_access_token: null
resources:
  accelerator: H100:8
  use_gpu: true
secrets:
  hf_access_token: null
runtime:
  predict_concurrency : 256
model_name: Llama 4 Maverick 17B 128E Instruct H100 TP8
