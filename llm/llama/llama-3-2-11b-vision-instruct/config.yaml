description: "Llama 3.2 11B Vision Instruct for vision-language tasks"
base_image:
  image: vllm/vllm-openai:v0.6.3.post1
model_metadata:
  tags:
    - openai-compatible
  repo_id: meta-llama/Llama-3.2-11B-Vision-Instruct
  example_model_input: {
    model: "llama-3.2-11b-vision-instruct",
    messages: [
      {
        role: "user",
        content: [
          {
            type: "text",
            text: "Describe this image in one sentence."
          },
          {
            type: "image_url",
            image_url: {
              url: "https://picsum.photos/id/237/200/300"
            }
          }
        ]
      }
    ],
    stream: true,
    max_tokens: 512,
    temperature: 0.5
  }
docker_server:
  start_command: sh -c "HF_TOKEN=$(cat /secrets/hf_access_token) vllm serve meta-llama/Llama-3.2-11B-Vision-Instruct --dtype half --served-model-name llama-3.2-11b-vision-instruct --tensor-parallel-size 1 --gpu-memory-utilization 0.90 --max-model-len 4000 --max-num-seqs 8 --distributed-executor-backend mp --disable-custom-all-reduce --use-v2-block-manager --trust-remote-code --enforce-eager"
  readiness_endpoint: /health
  liveness_endpoint: /health
  predict_endpoint: /v1/chat/completions
  server_port: 8000
resources:
  accelerator: A100
  use_gpu: true
model_name: Llama 3.2 11B Vision Instruct
secrets:
  hf_access_token: null
environment_variables:
  VLLM_LOGGING_LEVEL: WARNING
  hf_access_token: null
runtime:
  predict_concurrency: 64
