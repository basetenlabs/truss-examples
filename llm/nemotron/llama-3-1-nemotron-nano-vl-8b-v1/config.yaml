description: "Llama 3.1 Nemotron Nano VL 8B for vision-language tasks"
base_image:
  image: vllm/vllm-openai:v0.11.0
model_metadata:
  example_model_input: # Loads sample request into Baseten playground
    messages:
      - role: system
        content: "You are a helpful vision-language assistant."
      - role: user
        content:
          - type: image
            url: "https://upload.wikimedia.org/wikipedia/commons/f/fa/Grayscale_8bits_palette_sample_image.png"
          - type: text
            text: "Describe this image in detail."
    stream: true
    model: "nvidia/Llama-3.1-Nemotron-Nano-VL-8B-V1"
    max_tokens: 1024
    temperature: 0.7
  tags:
    - openai-compatible
model_name: Llama 3.1 Nemotron Nano VL 8B V1
requirements:
  - transformers>=4.55.0
  - accelerate==1.2.1
  - timm==1.0.12
  - einops==0.8.0
  - open-clip-torch==2.29.0
  - pillow==10.4.0
python_version: py312
model_cache:
  - repo_id: nvidia/Llama-3.1-Nemotron-Nano-VL-8B-V1
    revision: main
    use_volume: true
    volume_folder: "llama-3-1-nemotron-nano-vl-8b-v1"
    ignore_patterns:
      - "original/*"
      - "*.pth"
docker_server:
  start_command: vllm serve nvidia/Llama-3.1-Nemotron-Nano-VL-8B-V1 --tensor-parallel-size 1 --served-model-name llama-3-1-nemotron-nano-vl-8b-v1 --trust-remote-code --max-model-len 16384 --gpu-memory-utilization 0.9
  readiness_endpoint: /health
  liveness_endpoint: /health
  predict_endpoint: /v1/chat/completions
  server_port: 8000
resources:
  accelerator: H100:1
  use_gpu: true
runtime:
  predict_concurrency: 16
