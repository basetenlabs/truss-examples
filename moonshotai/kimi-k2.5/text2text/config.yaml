model_metadata:
  example_model_input:
    messages:
      - role: system
        content: "You are a helpful assistant."
      - role: user
        content: "What is the meaning of life?"
    stream: true
    model: moonshotai/Kimi-K2.5
    max_tokens: 32768
    temperature: 0.6
  tags:
    - openai-compatible

model_name: moonshotai/Kimi-K2.5

base_image:
  image: vllm/vllm-openai:cu130-nightly-5bff999d12dd061c102381b0c9c5d364c5953ea2

build_commands:
  - pip install --upgrade 'transformers>=5.2.0'

docker_server:
  start_command: >
    sh -c "SAFETENSORS_FAST_GPU=1 python3 -m vllm.entrypoints.openai.api_server
    --model /models/Kimi-K2.5
    --host 0.0.0.0 --port 8000
    --served-model-name moonshotai/Kimi-K2.5
    --tensor-parallel-size 8
    --language-model-only
    --trust-remote-code
    --load-format runai_streamer
    --disable-log-stats
    --max-num-seqs 64
    --max-num-batched-tokens 8192
    --reasoning-parser kimi_k2
    --enable-auto-tool-choice
    --tool-call-parser kimi_k2
    --enable-prefix-caching"
  readiness_endpoint: /health
  liveness_endpoint: /health
  predict_endpoint: /v1/chat/completions
  server_port: 8000

weights:
  - source: "hf://moonshotai/Kimi-K2.5@main"
    mount_location: "/models/Kimi-K2.5"
    ignore_patterns:
      - "*.md"
      - "*.txt"

resources:
  accelerator: B200:8
  use_gpu: true

runtime:
  predict_concurrency: 64
  health_checks:
    restart_check_delay_seconds: 1800
    restart_threshold_seconds: 1200
    stop_traffic_threshold_seconds: 120

environment_variables:
  VLLM_LOGGING_LEVEL: WARNING
