# Optimized Models

Production-grade, TensorRT-LLM optimized model configurations autogenerated by `_internal/templating/generate_templates.py`. These templates are not intended to be edited by hand.

| Directory | Models | Description |
|-----------|--------|-------------|
| [briton](briton/) | 33 | Baseten Runtime Optimized Networks -- TRT-LLM configs for Llama, Qwen, Gemma, DeepSeek, Mistral, Phi, and Falcon models with FP4/FP8 quantization and speculative decoding variants |
| [bisv2](bisv2/) | 11 | Baseten Inference Server v2 -- optimized serving configs for Llama, Qwen, DeepSeek, and NVIDIA models with FP4/FP8 quantization |

These configurations are generated from templates. To regenerate them, run:

```bash
python _internal/templating/generate_templates.py
```

## Deploying

Each optimized model can be deployed to Baseten with:

```bash
truss push
```
