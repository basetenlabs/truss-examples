build_commands:
  - apt-get update && apt-get install git git-lfs -y
  - git lfs install
  - git clone https://huggingface.co/hubertsiuzdak/snac_24khz /app/snac_24khz
environment_variables:
  ENABLE_EXECUTOR_API: "1"
model_metadata:
  repo_id: canopylabs/orpheus-3b-0.1-ft
  example_model_input:
    max_tokens: 10000
    prompt:
      "In todays fast-paced world, finding balance between work and personal
      life is more important than ever. With the constant demands of technology, remote
      communication, "
    voice: tara
  tags:
    - force-legacy-api-non-openai-compatible
model_name: Orpheus-3b
python_version: py39
requirements:
  - --extra-index-url https://download.pytorch.org/whl/cu128
  - torch==2.7.1
  - snac==1.2.1
  - batched==0.1.4
resources:
  # NOTE: Model is bottlenecked by CPU clock speed
  # H100 upgrade is not really effective
  accelerator: H100_40GB
  cpu: "1"
  memory: 10Gi
  use_gpu: true
secrets:
  hf_access_token: null
trt_llm:
  build:
    base_model: decoder
    checkpoint_repository:
      repo: baseten/orpheus-3b-0.1-ft
      revision: b9eb57a06083cb9e5a083885fad991aa79c0bd24
      source: HF
    max_batch_size: 256
    # set higher, so we can always use the max batch size in a single iter.
    max_num_tokens: 16384
    # 32768 would be around 300s of audio, typically model produces max 120s.
    max_seq_len: 32768
    num_builder_gpus: 1
    quantization_config:
      # TODO: Generate a typical dataset (input + output tokens) in target language
      # or disable quantization for other languages
      calib_dataset: "cnn_dailymail"
    plugin_configuration:
      use_fp8_context_fmha: true
    quantization_type: fp8_kv
    tensor_parallel_count: 1
  runtime:
    enable_chunked_context: true
    kv_cache_free_gpu_mem_fraction: 0.90
    batch_scheduler_policy: max_utilization
