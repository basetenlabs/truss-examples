base_image:
  image: public.ecr.aws/q9t5s3a7/vllm-ci-postmerge-repo:78336a0c3ee4eb9dba6e37959d926160e91623fd
build_commands:
  - pip install --pre --upgrade transformers
  - pip uninstall -y vllm
  - VLLM_USE_PRECOMPILED=1 VLLM_TEST_USE_PRECOMPILED_NIGHTLY_WHEEL=1 pip install git+https://github.com/vllm-project/vllm.git@78336a0c3ee4eb9dba6e37959d926160e91623fd
model_metadata:
  repo_id: ByteDance-Seed/Seed-OSS-36B-Instruct
  example_model_input: # Loads sample request into Baseten playground
    messages:
      - role: system
        content: "You are a helpful assistant."
      - role: user
        content: "Write FizzBuzz in Python"
    stream: true
    model: "ByteDance-Seed/Seed-OSS-36B-Instruct"
    max_tokens: 4096
    temperature: 0.6
  tags:
    - openai-compatible
docker_server:
  start_command: python3 -m vllm.entrypoints.openai.api_server --model  ByteDance-Seed/Seed-OSS-36B-Instruct -O3 --tensor-parallel-size 2 --tool-call-parser seed_oss --served-model-name ByteDance-Seed/Seed-OSS-36B-Instruct --enable-auto-tool-choice --max-model-len 65536 --gpu-memory-utilization=0.95 --host 0.0.0.0 --port 8000
  readiness_endpoint: /health
  liveness_endpoint: /health
  predict_endpoint: /v1/chat/completions
  server_port: 8000
resources:
  accelerator: H100:2
  use_gpu: true
runtime:
  predict_concurrency: 128
model_cache:
  - repo_id: ByteDance-Seed/Seed-OSS-36B-Instruct
    revision: 497f1dca95ebdec98e41d517b9f060ee753c902f
    use_volume: true
    volume_folder: glm
model_name: Seed-OSS-36B-Instruct
