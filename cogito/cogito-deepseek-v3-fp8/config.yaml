base_image:
  image: vllm/vllm-openai:v0.9.2
model_metadata:
  example_model_input: {
    model: "",
    messages: [
      {
        role: "user",
        content: "What do you think is the fate of the universe? Is there other intelligent life out there?"
      }
    ],
    stream: true,
    max_tokens: 512,
    temperature: 0.9
  }
  repo_id: deepcogito/cogito-v2-preview-deepseek-671B-MoE-FP8
  tags:
    - openai-compatible
docker_server:
  start_command: sh -c "HF_TOKEN=$(cat /secrets/hf_access_token) vllm serve deepcogito/cogito-v2-preview-deepseek-671B-MoE-FP8 --max-model-len 131072 --port 8000 --gpu-memory-utilization 0.90 --disable-custom-all-reduce --trust-remote-code --tensor-parallel-size 8 --distributed-executor-backend mp --enable-auto-tool-choice --tool-call-parser deepseek_v3"
  readiness_endpoint: /health
  liveness_endpoint: /health
  predict_endpoint: /v1/chat/completions
  server_port: 8000
cache_internal:
  - repo_id: deepcogito/cogito-v2-preview-deepseek-671B-MoE-FP8
resources:
  accelerator: B200:8
  cpu: '1'
  memory: 24Gi
  use_gpu: true
runtime:
  predict_concurrency : 32
model_name: DeepSeek V3 671B MoE FP8
environment_variables:
  hf_access_token: null
