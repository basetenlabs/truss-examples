base_image:
  image: vllm/vllm-openai:v0.10.0
model_metadata:
  repo_id: deepcogito/cogito-v2-preview-llama-109B-MoE
  example_model_input: {
    "model": "llama",
    "messages": [
      {
      "role": "user",
      "content": "Given an array of integers nums and an integer target, return indices of the two numbers such that they add up to target. You may assume that each input would have exactly one solution, and you may not use the same element twice. You can return the answer in any order. class Solution: def twoSum(self, nums: List[int], target: int) -> List[int]:"
      }
    ],
    "stream": true,
    "max_tokens": 512,
    "temperature": 0.5
  }
  tags:
  - openai-compatible
docker_server:
  start_command: sh -c "HF_TOKEN=$(cat /secrets/hf_access_token) vllm serve deepcogito/cogito-v2-preview-llama-109B-MoE --served-model-name llama --max-model-len 32000 --tensor-parallel-size 4 --distributed-executor-backend mp --enable-auto-tool-choice --tool-call-parser llama3_json"
  readiness_endpoint: /health
  liveness_endpoint: /health
  predict_endpoint: /v1/chat/completions
  server_port: 8000
environment_variables:
  VLLM_LOGGING_LEVEL: INFO
  hf_access_token: null
resources:
  accelerator: H100:4
  use_gpu: true
secrets:
  hf_access_token: null
runtime:
  predict_concurrency : 256
model_name: Cogito V2 Preview Llama 109B MoE vLLM
