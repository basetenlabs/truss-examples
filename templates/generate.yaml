mistral/mistral-7b-trt-llm:
  based_on: trt-llm
  config:
    model_metadata:
      avatar_url: https://cdn.baseten.co/production/static/explore/mistral_logo.png
      cover_image_url: https://cdn.baseten.co/production/static/explore/mistral.png
      tensor_parallelism: 1
      engine_repository: "baseten/mistral7b-fp16-ifb-2k-2k"
      tokenizer_repository: "mistralai/Mistral-7B-v0.1"
    description: Generate text from a prompt with this seven billion parameter language model.
    model_name: Mistral 7B TRT
    resources:
      accelerator: A100
  ignore:
    - README.md
