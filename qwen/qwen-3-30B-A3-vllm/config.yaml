base_image:
  image: vllm/vllm-openai:v0.8.5
docker_server:
  start_command: sh -c "vllm serve Qwen/Qwen3-30B-A3B --enable-reasoning --reasoning-parser deepseek_r1 --served-model-name qwen30b --port 8000"
  readiness_endpoint: /health
  liveness_endpoint: /health
  predict_endpoint: /v1/chat/completions
  server_port: 8000
model_metadata:
  repo_id: Qwen/Qwen3-30B-A3B
  example_model_input:
    messages:
      - role: system
        content: "You are a helpful assistant."
      - role: user
        content: "What does Tongyi Qianwen mean?"
    stream: false
    model: "qwen30b"
    max_tokens: 512
    temperature: 0.7
  tags:
    - openai-compatible
resources:
  accelerator: H100:1
  use_gpu: true
runtime:
  predict_concurrency: 32
model_name: Qwen 3 30B-A3B vLLM
environment_variables:
  VLLM_LOGGING_LEVEL: WARNING
