model_metadata:
  example_model_input:
    messages:
      - role: system
        content: "You are a helpful assistant."
      - role: user
        content: "What is the meaning of life?"
    stream: true
    model: Qwen/Qwen3.5-397B-A17B
    max_tokens: 32768
    temperature: 0.6
  tags:
    - openai-compatible

model_name: Qwen/Qwen3.5-397B-A17B

base_image:
  image: vllm/vllm-openai:qwen3_5-cu130

docker_server:
  start_command: >
    sh -c "SAFETENSORS_FAST_GPU=1 python3 -m vllm.entrypoints.openai.api_server
    --model /models/Qwen3.5-397B-A17B
    --host 0.0.0.0 --port 8000
    --served-model-name Qwen/Qwen3.5-397B-A17B
    --tensor-parallel-size 8
    --language-model-only
    --trust-remote-code
    --load-format runai_streamer
    --disable-log-stats
    --max-num-seqs 64
    --max-num-batched-tokens 8192
    --reasoning-parser qwen3
    --enable-auto-tool-choice
    --tool-call-parser qwen3_coder
    --attention-backend FLASH_ATTN
    --speculative-config '{\"method\":\"mtp\",\"num_speculative_tokens\":1}'
    --enable-prefix-caching"
  readiness_endpoint: /health
  liveness_endpoint: /health
  predict_endpoint: /v1/chat/completions
  server_port: 8000

weights:
  - source: "hf://Qwen/Qwen3.5-397B-A17B@main"
    mount_location: "/models/Qwen3.5-397B-A17B"
    ignore_patterns:
      - "*.md"
      - "*.txt"

resources:
  accelerator: B200:8
  use_gpu: true

runtime:
  predict_concurrency: 64
  health_checks:
    restart_check_delay_seconds: 1800
    restart_threshold_seconds: 1200
    stop_traffic_threshold_seconds: 120

environment_variables:
  VLLM_LOGGING_LEVEL: WARNING
