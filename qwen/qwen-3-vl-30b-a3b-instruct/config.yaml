model_metadata:
  example_model_input: # Loads sample request into Baseten playground
    model: "Qwen/Qwen3-VL-30B-A3B-Thinking"
    stream: false
    max_tokens: 4096
    messages:
      - role: user
        content:
          - type: text
            text: "What's in this image?"
          - type: image_url
            image_url:
              url: "https://github.com/sgl-project/sglang/blob/main/test/lang/example_image.png?raw=true"
    temperature: 0.6
  tags:
    - openai-compatible
model_name: Qwen3-VL-30B-A3B-Instruct-FP8
base_image:
  image: public.ecr.aws/q9t5s3a7/vllm-ci-postmerge-repo:2f7dbc9b42c51ba192e3dded515e4e07cdfdabea
build_commands:
  - pip install --pre --upgrade transformers
  - pip uninstall -y vllm
  - VLLM_USE_PRECOMPILED=1 VLLM_TEST_USE_PRECOMPILED_NIGHTLY_WHEEL=1 pip install git+https://github.com/vllm-project/vllm.git@d3d649efec8161b62e8db576f8d1d02a77d22897
docker_server:
  start_command: python3 -m vllm.entrypoints.openai.api_server --model Qwen/Qwen3-VL-30B-A3B-Instruct-FP8 --tool-call-parser hermes --reasoning-parser qwen3 --served-model-name Qwen/Qwen3-VL-30B-A3B-Instruct --enable-expert-parallel --enable-auto-tool-choice --tensor-parallel-size 2 --host 0.0.0.0 --port 8000
  readiness_endpoint: /health
  liveness_endpoint: /health
  predict_endpoint: /v1/chat/completions
  server_port: 8000
resources:
  accelerator: H100:2
  use_gpu: true
runtime:
  predict_concurrency: 32
