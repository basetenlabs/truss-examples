model_metadata:
  example_model_input:
    model: "Qwen/Qwen3-VL-32B-Instruct-FP8"
    messages:
      - role: user
        content:
          - type: image_url
            image_url:
              url: "data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mNk+M9QDwADhgGAWjR9awAAAABJRU5ErkJggg=="
          - type: text
            text: "<|grounding|>Convert the document to markdown."
    max_tokens: 4096
    temperature: 0.6
    scale: 1.0  # This can be overridden per-request
  openai_compatible: true
  tags:
    - openai-compatible
model_name: Qwen3-VL-32B-Instruct-FP8-Test
environment_variables:
  # Use B10fs org-level persistent storage for DeepGEMM cache
  # This persists across all replicas and deployments
  SGLANG_DG_CACHE_DIR: "/cache/org/deep_gemm_qwen3_vl_32b_tp2"
model_cache:
  - repo_id: Qwen/Qwen3-VL-32B-Instruct-FP8
    revision: main
    use_volume: true
    volume_folder: "qwen3-vl-32b"
    ignore_patterns:
      - "original/*"
      - "*.pth"
base_image:
  image: lmsysorg/sglang:v0.5.5.post3
requirements:
  - sglang
  - httpx==0.25.2
  - Pillow==10.1.0
  - fastapi
resources:
  accelerator: H100:2
  use_gpu: true
secrets:
  hf_access_token: null
runtime:
  predict_concurrency: 256