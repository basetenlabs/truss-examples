model_name: Qwen3.5-35B-A3B vLLM
model_metadata:
  example_model_input:
    model: "Qwen/Qwen3.5-35B-A3B"
    messages:
      - role: user
        content: "What is the capital of France?"
    max_tokens: 100
    temperature: 0.7
base_image:
  image: vllm/vllm-openai:qwen3_5
weights:
  - source: "hf://Qwen/Qwen3.5-35B-A3B@b1fc3d59ae0ab1e4279e04a8dd0fc4dc361fc2b6"
    mount_location: "/app/model_cache/qwen3.5-35b-a3b"
    auth_secret_name: "hf_access_token"
build_commands: []
docker_server:
  start_command: >-
    sh -c "GPU_COUNT=$(nvidia-smi --list-gpus | wc -l);
    vllm serve /app/model_cache/qwen3.5-35b-a3b
    --served-model-name Qwen/Qwen3.5-35B-A3B
    --host 0.0.0.0
    --port 8000
    --tensor-parallel-size $GPU_COUNT
    --gpu-memory-utilization 0.95
    --max-model-len 262144
    --dtype bfloat16
    --reasoning-parser qwen3"
  readiness_endpoint: /health
  liveness_endpoint: /health
  predict_endpoint: /v1/chat/completions
  server_port: 8000
runtime:
  predict_concurrency: 128
resources:
  accelerator: H100:4
  use_gpu: true
secrets:
  hf_access_token: null
