model_metadata:
  example_model_input: # Loads sample request into Baseten playground
    messages:
      - role: system
        content: "You are a helpful assistant."
      - role: user
        content: "What does Tongyi Qianwen mean?"
    stream: false
    model: "Qwen/Qwen3-235B-A22B-Instruct-2507-FP8"
    max_tokens: 512
    temperature: 0.6
  tags:
    - openai-compatible
  repo_id: Qwen/Qwen3-235B-A22B-Instruct-2507-FP8
model_name: Qwen/Qwen3-235B-A22B-Instruct-2507-FP8
model_cache:
  - repo_id: Qwen/Qwen3-235B-A22B-Instruct-2507-FP8
    use_volume: true
    revision: main
    volume_folder: trt_model
resources:
  accelerator: H100:8
  cpu: "1"
  memory: 10Gi
  use_gpu: true
trt_llm:
  build:
    checkpoint_repository:
      repo: michaelfeil/empty-model
      revision: main
      source: HF
  inference_stack: v2
  runtime:
    enable_chunked_prefill: true
    max_batch_size: 256
    max_num_tokens: 8192
    max_seq_len: 262144
    served_model_name: Qwen/Qwen3-235B-A22B-Instruct-2507-FP8
    tensor_parallel_size: 8
    patch_kwargs:
      disable_overlap_scheduler: True
      model_path: /app/model_cache/trt_model
      moe_expert_parallel_size: 8
      cuda_graph_config:
        enable_padding: true
        max_batch_size: 256
      enable_autotune: false
      guided_decoding_backend: "xgrammar"
      enable_iter_perf_stats: 0
      kv_cache_config:
        enable_block_reuse: true
        free_gpu_memory_fraction: 0.8
  version_overrides:
    v2_llm_version: trtllm-gpu-1.2.0rc0-30f65c3c-451c407f08
