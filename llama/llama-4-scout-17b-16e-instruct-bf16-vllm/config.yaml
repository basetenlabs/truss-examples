base_image:
  image: vllm/vllm-openai:v0.8.4
build_commands: 
  - pip install git+https://github.com/huggingface/transformers.git hf-xet
model_metadata:
  repo_id: meta-llama/Llama-4-Scout-17B-16E-Instruct
  example_model_input: {
    "model": "llama",
    "messages": [
      {
      "role": "user",
      "content": "Given an array of integers nums and an integer target, return indices of the two numbers such that they add up to target. You may assume that each input would have exactly one solution, and you may not use the same element twice. You can return the answer in any order. class Solution: def twoSum(self, nums: List[int], target: int) -> List[int]:"
      }
    ],
    "stream": true,
    "max_tokens": 512,
    "temperature": 0.5
  }
  tags:
  - openai-compatible
docker_server:
  start_command: sh -c /app/data/do.sh
  readiness_endpoint: /health
  liveness_endpoint: /health
  predict_endpoint: /v1/chat/completions
  server_port: 8000
environment_variables:
  hf_access_token: null
resources:
  accelerator: H100:4
  use_gpu: true
secrets:
  hf_access_token: null
runtime:
  predict_concurrency : 256
model_name: Llama 4 Scout 17B 16E Instruct H100 TP 4
